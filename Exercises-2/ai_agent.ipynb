{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2024-05-14T22:16:56+02:00', 'author': 'BMW AG', 'moddate': '2024-05-17T08:49:01+02:00', 'subject': 'Speech of the CEO Oliver Zipse (full text)', 'title': 'Speech of the CEO Oliver Zipse (full text)', 'source': '../input/Zipse_Speech.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2'}\n",
      "82\n",
      "gpt-4o-mini\n",
      "{'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2024-05-14T22:16:56+02:00', 'author': 'BMW AG', 'moddate': '2024-05-17T08:49:01+02:00', 'subject': 'Speech of the CEO Oliver Zipse (full text)', 'title': 'Speech of the CEO Oliver Zipse (full text)', 'source': '../input/Zipse_Speech.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2'}\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "%run ../Exercises-1/rag_qna_bot.ipynb\n",
    "%run ../Exercises-1/summarize_bot.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os,json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Keys\n",
    "azure_endpoint: str = os.environ.get('AZURE_ENDPOINT')\n",
    "azure_openai_api_key: str = os.environ.get('AZURE_OPENAI_API_KEY')\n",
    "azure_openai_api_version: str = os.environ.get('AZURE_OPENAI_API_VERSION')\n",
    "azure_deployment: str = os.environ.get('AZURE_DEPLOYMENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint = azure_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intent(user_input):\n",
    "    # Define the system prompt for intent classification\n",
    "    system_prompt = \"\"\"\n",
    "    You are an intent classification system. Your task is to classify user inputs into one of two categories:\n",
    "    1. Q&A: The user is asking a question and expects an answer.\n",
    "    2. Summary: The user is asking for a summary of content.\n",
    "    \n",
    "    For each input, return a JSON object with:\n",
    "    - \"intent\": Either \"Q&A\" or \"Summary\"\n",
    "    - \"confidence\": A score between 0 and 1 indicating your confidence\n",
    "    - \"explanation\": A brief explanation of why you classified it this way\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create messages for the API call\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    \n",
    "    # Call Azure OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        temperature=0.1,  # Low temperature for more deterministic responses\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    # Parse the response\n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_content_policy(user_input):\n",
    "    system_prompt = \"\"\"\n",
    "    You are a content policy checker. Your task is to determine if the user input contains sensitive topics \n",
    "    that should be avoided in responses. The topics to avoid are:\n",
    "    \n",
    "    - Religion: Religious discussions, debates about faith, religious practices, criticism of religions\n",
    "    - Adult content: Explicit sexual content, pornography, sexual acts\n",
    "    - Illegal activities: Instructions for illegal activities, promotion of criminal behavior\n",
    "    - Weapons and violence: Detailed instructions about weapons, promotion of violence\n",
    "    - Medical advice: Specific medical diagnoses, treatment recommendations, health advice\n",
    "    - Financial investment advice: Specific investment recommendations, financial predictions\n",
    "    - Personal identifiable information: Requests for or sharing of personal data\n",
    "    \n",
    "    Return with a JSON object containing:\n",
    "    - \"is_allowed\": boolean (True if content is allowed, False if it contains sensitive topics)\n",
    "    - \"reason\": string explaining why the content is not allowed (if applicable)\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat_bot(question):\n",
    "  response=classify_intent(question)\n",
    "  print(f'Intent Identified By AI : {response.get('intent')}')\n",
    "  if response.get('intent') == 'Q&A':\n",
    "    response=ask_question(question)\n",
    "    policy_json=check_content_policy(response)\n",
    "    if policy_json.get('is_allowed'):\n",
    "      return response\n",
    "    else:\n",
    "      return policy_json.get('reason')\n",
    "  elif response.get('intent') == 'Summary':\n",
    "    response=run_summary()\n",
    "    policy_json=check_content_policy(response)\n",
    "    if policy_json.get('is_allowed'):\n",
    "      return response\n",
    "    else:\n",
    "      return policy_json.get('reason')\n",
    "  else:\n",
    "    return 'Could not Find Intent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Identified By AI : Q&A\n",
      "| Event                                    | Date              |\n",
      "|------------------------------------------|-------------------|\n",
      "| 104th Annual General Meeting of BMW AG  | 15th May 2024     |\n"
     ]
    }
   ],
   "source": [
    "question = 'when the speech happned'\n",
    "print(chat_bot(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
